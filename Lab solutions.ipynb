{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bda7c94e-da69-44ad-bb29-ee1f4b1b8668",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "# Spatial SQL functions workshop exercise\n",
    "\n",
    "## Assessing wildfire exposure for commercial property risks\n",
    "\n",
    "<img src=\"assets/wildfire.png\">\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "28b9d5e3-f469-4c72-a004-dd18c2baefa6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Installation of dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "87cc3277-4a47-482a-b33d-34ca01d78226",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt\n",
    "%restart_python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c9947503-4e02-4b80-b4c0-51ea68082311",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "import geopandas as gpd\n",
    "import math\n",
    "import shapely\n",
    "\n",
    "from lonboard import Map, PolygonLayer, ScatterplotLayer\n",
    "\n",
    "import pyspark.sql\n",
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3aefdbbc-6ad6-4065-99b4-6eb85e11d4fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Configuration\n",
    "Replace these values with your own"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0033f46f-16a0-4eff-906e-009a5f9b145e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "user_email = spark.sql(\"SELECT CURRENT_USER() AS u\").first()[\"u\"]\n",
    "user_name = user_email.split(\"@\")[0].replace(\".\", \"_\")\n",
    "CATALOG = user_name\n",
    "SCHEMA = \"geoday\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07ab6b4b-4476-45d5-b84e-11819b022cf7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"CREATE SCHEMA IF NOT EXISTS {CATALOG}.{SCHEMA}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ca08487-4ff3-401c-8b04-ac0c77ce91fc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.conf.set(\"spark.databricks.geo.st.enabled\", \"true\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7440613e-aa1f-49c7-b908-7f6f33c04c67",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Overview of available datasets\n",
    "\n",
    "### Overture Maps Buildings\n",
    "+ **Description**: Building footprint dataset. Contains building footprint polygons with some basic characteristics of the building where available.\n",
    "+ **Scope**: Global\n",
    "+ **Provider**: [Overture Maps Foundation](https://overturemaps.org/) (via [CARTO](https://carto.com/))\n",
    "+ **Marketplace listing**: [⧉](https://marketplace.databricks.com/details/ccacdfa3-b85d-4065-bd70-efa673c197e1/CARTO_Overture-Maps-Buildings)\n",
    "\n",
    "### Overture Maps Places\n",
    "+ **Description**: Addresses and locations (as point geometries) of points-of-interest including landmarks, businesses, transport hubs etc. with their names, functions and similar attributes.\n",
    "+ **Scope**: Global\n",
    "+ **Provider**: [Overture Maps Foundation](https://overturemaps.org/) (via [CARTO](https://carto.com/))\n",
    "+ **Marketplace listing**: [⧉](https://marketplace.databricks.com/details/aca03de7-a670-4e3f-8ee8-b0f319f0a247/CARTO_Overture-Maps-Places)\n",
    "\n",
    "### Wildfire Risk USA (sample)\n",
    "+ **Description**: Wildfire hazard and risk assessment database. Identifies uniquely generated FireShed polygons covering the United States with corresponding risk scores. Selection of models used to compute risks depending population density (Wildland / Intermix / Interface).\n",
    "+ **Scope**: USA (Marketplace sample is limited to Marin County, CA)\n",
    "+ **Provider**: [Precisely](https://www.precisely.com/)\n",
    "+ **Marketplace listing**: [⧉](https://marketplace.databricks.com/details/7ef74135-019f-4ec4-9c91-caf9b50233af/Precisely_Wildfire-Risk-USA-Sample)\n",
    "\n",
    "### ECMWF 15-day operational weather forecast\n",
    "+ **Description**: Detailed weather forecasts that are produced by the Integrated Forecasting System (IFS) (i.e. the 'physical' weather model) maintained by the [European Centre for Medium-Range Weather Forecasts (ECMWF)](https://www.ecmwf.int/). These operational forecasts are the main high-resolution deterministic forecast produced by ECMWF and extend out to the next 15-days ahead, initially in 3-hour timesteps (for the first five days), moving to 6-hour steps thereafter. They are published as [GRIB messages](https://confluence.ecmwf.int/display/CKB/What+are+GRIB+files+and+how+can+I+read+them#WhatareGRIBfilesandhowcanIreadthem-HowtoreadGRIBfiles) for a large number of weather variables for points in a regular 0.25° grid.\n",
    "+ **Scope**: Global\n",
    "+ **Provider**: [ECMWF]()\n",
    "+ **Marketplace listing**: N/A (will be downloaded) [terms of use](https://apps.ecmwf.int/datasets/licences/general/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0624c137-837f-40e7-b0c5-9a1ce96a297a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Getting started\n",
    "\n",
    "First task: get access to the above Marketplace datasets.\n",
    "1) Follow the link to each asset's Marketplace listing;\n",
    "2) Click \"Get Access\" and login with your Databricks credentials; and\n",
    "3) Follow the instructions to deploy the asset into a new Catalog in your Unity Catalog metastore.\n",
    "\n",
    "When complete, run the following cell to check these have been correctly deployed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e4b9246b-e5de-4179-91e8-92c7f79ab3fa",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Check marketplace catalogs exist"
    }
   },
   "outputs": [],
   "source": [
    "def catalog_exists(catalog_name):\n",
    "  try:\n",
    "    spark.sql(f\"SHOW CATALOGS LIKE '{catalog_name}'\")\n",
    "    return True\n",
    "  except:\n",
    "    return False\n",
    "\n",
    "assert all(catalog_exists(c) for c in [\n",
    "  \"precisely_wildfire_risk_usa_sample\",\n",
    "  \"carto_overture_maps_places\",\n",
    "  \"carto_overture_maps_buildings\"\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f53046d6-6fe1-4db0-a916-4fe5bcbf1131",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## The task\n",
    "In this task, we aim to build a very simplistic model of wildfire risk for all of the commercial property in Marin County, CA.\n",
    "\n",
    "Though simplistic, this model will account for the frequency and severity of historical fires, the physical distance between the property and a nearby fire-station and, as an optional extension, the outlook for weather in the region.\n",
    "\n",
    "In order to achieve this, we will need to perform a number of subtasks:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee4cbee1-e2b7-4252-8e12-dee2ee60538b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Sub-task (1): identify points-of-interest (commercial property risks)\n",
    "+ Extract the building footprints for all _commercial_ properties in the **Overture Maps Buildings** dataset that fall within the zone covered by the **Wildfire Risk USA** data.\n",
    "\n",
    "+ This will require aggregating (unioning) the FireShed zones in the table `precisely_wildfire_risk_usa_sample.riskdata.wfr_usa_fireriskpro` into one large polygon and computing the bounding box of said polygon.\n",
    "\n",
    "+ The parameters of this bounding box can then be used to filter the **Overture Maps Buildings** dataset stored at `carto_overture_maps_buildings.carto.building`.\n",
    "  + **Hint** This table contains a number of fields prefixed with `__carto_` that describe the bounding box of each building footprint.\n",
    "\n",
    "+ Use the provided code to plot a selection of these on a map using the `lonboard` package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "db95b889-418b-46ef-a00e-3311ca65b1fe",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM precisely_wildfire_risk_usa_sample.riskdata.wfr_usa_fireriskpro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d73ded29-60e3-4fe2-a9ef-38e0c9fce9e4",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Solution 1.1"
    }
   },
   "outputs": [],
   "source": [
    "fireriskpro_tref = \"precisely_wildfire_risk_usa_sample.riskdata.wfr_usa_fireriskpro\"\n",
    "\n",
    "xmin, xmax, ymin, ymax = (\n",
    "  spark.table(fireriskpro_tref)\n",
    "  .select(F.expr(\"st_aswkb(st_union_agg(WKT))\").alias(\"geom\"))\n",
    "  .select(\n",
    "    F.expr(\"st_xmin(geom)\").alias(\"xmin\"),\n",
    "    F.expr(\"st_xmax(geom)\").alias(\"xmax\"),\n",
    "    F.expr(\"st_ymin(geom)\").alias(\"ymin\"),\n",
    "    F.expr(\"st_ymax(geom)\").alias(\"ymax\"),\n",
    "  )\n",
    ").first()\n",
    "\n",
    "print(\n",
    "  \"Fire risk pro dataset bounds:\\n\"\n",
    "  f\"{xmin=}, {xmax=}, {ymin=}, {ymax=}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e5f5252-5c0c-4a21-a1bc-c1642e3c58d1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "buildings_tref = f\"{CATALOG}.{SCHEMA}.buildings\"\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {buildings_tref}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "70e99d59-a72a-491e-acf7-8d7202908899",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Solution 1.2"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {buildings_tref} AS\n",
    "SELECT\n",
    "  id,\n",
    "  geometry,\n",
    "  class,\n",
    "  subtype,\n",
    "  height,\n",
    "  variant_get(names, '$.primary', 'string') as building_name\n",
    "FROM\n",
    "  carto_overture_maps_buildings.carto.building\n",
    "WHERE\n",
    "  class = 'commercial'\n",
    "  AND __carto_xmin >= {xmin}\n",
    "  AND __carto_xmax <= {xmax}\n",
    "  AND __carto_ymin >= {ymin}\n",
    "  AND __carto_ymax <= {ymax}\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1dcf1bd4-a05d-4538-9f22-465d7f058d8f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "buildings_df = spark.table(buildings_tref)\n",
    "building_cols = buildings_df.columns\n",
    "print(f\"{buildings_df.count()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cbaf6fa4-ea32-46d3-bb43-97d07f64cd89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(buildings_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d78ba9f0-9b32-4dc3-9a5a-f31e0918f533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "buildings_pdf = buildings_df.toPandas()\n",
    "buildings_pdf[\"geometry\"] = gpd.GeoSeries.from_wkb(\n",
    "  buildings_pdf.geometry, crs=\"EPSG:4326\"\n",
    "  )\n",
    "buildings_gdf = gpd.GeoDataFrame(buildings_pdf, geometry=\"geometry\")\n",
    "\n",
    "m = Map([PolygonLayer.from_geopandas(\n",
    "    buildings_gdf, get_fill_color=[255, 0, 0]\n",
    ")])\n",
    "\n",
    "displayHTML(m.to_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "053770b0-b006-450d-bb52-aaa7a4a1db1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Sub-task (2): identify fire station locations in the same area\n",
    "\n",
    "+ Now, query the **Overture Maps Places** dataset to find the locations of local fire stations.\n",
    "  + In order to extract e.g. primary usage category from the VariantType field `categories`, you'll need to employ the Spark expression `variant_get` like so:\n",
    "  `variant_get(categories, '$.primary', 'string')`\n",
    "  + Since we want to compute the distance from each property in our analysis 'set', you might want to 'buffer' the bounding box before applying it to the **Places** data.\n",
    "\n",
    "+ As before, you can use the provided code to plot these locations on a map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47326975-d9d2-4a44-8ba4-573d5bbaf0ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "firestations_tref = f\"{CATALOG}.{SCHEMA}.firestations\"\n",
    "\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {firestations_tref}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dd516406-dbe4-4b9c-8fcb-cb1cd20019c6",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Solution 2.1"
    }
   },
   "outputs": [],
   "source": [
    "buffer = 0.05 # buffer around area of interest in degrees (~5km)\n",
    "\n",
    "spark.sql(f\"\"\"\n",
    "CREATE OR REPLACE TABLE {firestations_tref} AS\n",
    "WITH ft AS\n",
    "(\n",
    "  SELECT\n",
    "    id,\n",
    "    geometry,\n",
    "    variant_get(names, '$.primary', 'string') AS firestation_name,\n",
    "    variant_get(categories, '$.primary', 'string') AS firestation_primary_category,\n",
    "    variant_get(addresses, '$.list[0].element', 'map<string, string>') AS address_map\n",
    "  FROM\n",
    "    carto_overture_maps_places.carto.place\n",
    "  WHERE\n",
    "    variant_get(categories, '$.primary', 'string') = 'fire_department'\n",
    "    AND __carto_xmin >= {xmin} - {buffer}\n",
    "    AND __carto_xmax <= {xmax} + {buffer}\n",
    "    AND __carto_ymin >= {ymin} - {buffer}\n",
    "    AND __carto_ymax <= {ymax} + {buffer}\n",
    ")\n",
    "SELECT\n",
    "  ft.id as firestation_building_id,\n",
    "  ft.geometry as firestation_geometry,\n",
    "  ft.firestation_name,\n",
    "  ft.firestation_primary_category,\n",
    "  named_struct(\n",
    "    'freeform', ft.address_map['freeform'],\n",
    "    'locality', ft.address_map['locality'],\n",
    "    'region', ft.address_map['region'],\n",
    "    'postcode', ft.address_map['postcode'],\n",
    "    'country', ft.address_map['country']\n",
    "    ) as address\n",
    "  FROM ft\n",
    "  \"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "feb34ca7-c5f0-4562-9eb8-0c58f270db30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "firestations_df = spark.table(firestations_tref)\n",
    "firestation_cols = firestations_df.columns\n",
    "print(f\"{firestations_df.count()=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1da98d90-08cc-451e-a9d8-a47fdc2b5446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(firestations_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7b2dbd28-6337-4b48-ae72-277fe0311c23",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "firestations_pdf = firestations_df.toPandas()\n",
    "firestations_pdf[\"geom\"] = gpd.GeoSeries.from_wkb(\n",
    "  firestations_pdf.firestation_geometry, crs=\"EPSG:4326\"\n",
    "  )\n",
    "firestations_gdf = gpd.GeoDataFrame(firestations_pdf, geometry=\"geom\")\n",
    "\n",
    "point_layer = ScatterplotLayer.from_geopandas(\n",
    "    firestations_gdf, radius_scale=100, get_fill_color=[0, 255, 0],\n",
    "    get_line_color=[0, 0, 0], stroked=True\n",
    ")\n",
    "m = Map([point_layer])\n",
    "\n",
    "displayHTML(m.to_html())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4642da37-e314-4507-8bbd-8636fb4c278f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Sub-task (3): compute the \"as-the-crow-flies\" distance from each property to its nearest fire station\n",
    "\n",
    "+ Since our dataset is small, we can do this naively i.e. compute the distance (in metres if possible) between every property and every fire station.\n",
    "  + **Hint** The `st_distance` function returns results in the same units as its inputs, e.g. degrees for geometries expressed in geographic coordinates and metres for those in cartesian coordinates.\n",
    "  + In order to transform from geographic to cartesian coordinates, we can use the `st_transform` method. You'll need to find a suitable projected CRS to transform them into, e.g. `Universal Transverse Mercator Zone 10 North` [⧉](https://spatialreference.org/ref/epsg/32610/).\n",
    "\n",
    "+ We can then filter the results set to only keep the closest option of those computed. Applying a window function that uses `row_number()` is a usually a good approach here.\n",
    "  + **Hint** Get the assistant to help you define the window specification if you get stuck.\n",
    "\n",
    "+ Pick a fire station and map the properties that are in its 'catchment', i.e. for whom this is their nearest option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16c66eb4-bbc3-4804-9c47-ad202917beab",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Solution 3.1a"
    }
   },
   "outputs": [],
   "source": [
    "def add_utm10n_geom(df: pyspark.sql.DataFrame, col: str) -> pyspark.sql.DataFrame:\n",
    "    newcol = f\"{col}_utm10N\"\n",
    "    op = F.expr(f\"st_aswkb(st_transform({col}, 32610))\")\n",
    "    return df.withColumn(newcol, op)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd4fb946-1b72-4b15-ae53-b10cc762a0cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    },
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "buildings_projected_tref = f\"{CATALOG}.{SCHEMA}.buildings_projected\"\n",
    "\n",
    "(\n",
    "    buildings_df\n",
    "    .transform(add_utm10n_geom, \"geometry\")\n",
    "    .write.saveAsTable(buildings_projected_tref, mode=\"overwrite\")\n",
    ")\n",
    "\n",
    "buildings_projected_df = spark.table(buildings_projected_tref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0fbeb2a6-e5f6-4aee-81af-0e219c23d58c",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Solution 3.1b"
    }
   },
   "outputs": [],
   "source": [
    "# all_firestations_tref = f\"{CATALOG}.{SCHEMA}.all_firestations\"\n",
    "\n",
    "distance_to_all_firestations = (\n",
    "    buildings_projected_df\n",
    "    .alias(\"b\")\n",
    "    .crossJoin(\n",
    "        firestations_df.transform(add_utm10n_geom, \"firestation_geometry\").alias(\"f\")\n",
    "        )\n",
    "    .withColumn(\"firestation_distance\", F.expr(\"st_distance(geometry_utm10N, firestation_geometry_utm10N)\"))\n",
    "    .selectExpr(\"* EXCEPT (geometry_utm10N, firestation_geometry_utm10N)\")\n",
    "    # .write\n",
    "    # .saveAsTable(all_firestations_tref, mode=\"overwrite\")\n",
    ")\n",
    "\n",
    "# distance_to_all_firestations = spark.table(all_firestations_tref)\n",
    "\n",
    "display(distance_to_all_firestations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5fbaea7-6f62-4306-b06f-77107f3f1512",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Solution 3.2"
    }
   },
   "outputs": [],
   "source": [
    "ws_distance = Window.partitionBy(\"id\").orderBy(\"firestation_distance\")\n",
    "\n",
    "closest_firestation = (\n",
    "  distance_to_all_firestations\n",
    "  .withColumn(\"rank\", F.row_number().over(ws_distance))\n",
    "  .where(F.col(\"rank\") == 1)\n",
    "  .drop(\"rank\")\n",
    ")\n",
    "\n",
    "display(closest_firestation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "79a1f68d-6fc1-4952-97bf-cdd5378e15f4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def visualize_firestation(name: str):\n",
    "    filtered_pdf = (\n",
    "        closest_firestation.where(F.col(\"firestation_name\") == name)\n",
    "        .select(*building_cols, \"firestation_distance\")\n",
    "        .toPandas()\n",
    "    )\n",
    "\n",
    "    filtered_pdf[\"geometry\"] = gpd.GeoSeries.from_wkb(filtered_pdf[\"geometry\"])\n",
    "    gdf = gpd.GeoDataFrame(filtered_pdf, geometry=\"geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "    # Filter for the firestation name\n",
    "    firestation_pdf = (\n",
    "        firestations_df.select(*firestation_cols)\n",
    "        .where(F.col(\"firestation_name\") == name)\n",
    "        .toPandas()\n",
    "    )\n",
    "\n",
    "    firestation_pdf[\"firestation_geometry\"] = gpd.GeoSeries.from_wkb(firestation_pdf[\"firestation_geometry\"])\n",
    "    gdf_point = gpd.GeoDataFrame(firestation_pdf, geometry=\"firestation_geometry\", crs=\"EPSG:4326\")\n",
    "\n",
    "    m = Map(\n",
    "        [\n",
    "            ScatterplotLayer.from_geopandas(\n",
    "                gdf_point, radius_scale=15, get_fill_color=[255, 0, 0]\n",
    "            ),\n",
    "            PolygonLayer.from_geopandas(\n",
    "                gdf,\n",
    "                get_fill_color=[0, 0, 255],\n",
    "                extruded=True,\n",
    "                get_elevation=gdf[\"height\"].fillna(1),\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "    m.set_view_state(pitch=59.0)\n",
    "    displayHTML(m.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdf71ed4-5e75-4fa1-9a4c-a2689ceea41a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "visualize_firestation(\"Southern Marin Fire Station No. 7\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0d7816eb-1f5b-4b4f-b9f7-b6b9a7e1ae7e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Sub-task (4): overlay the risk parameters for each property based on its fireshed location\n",
    "\n",
    "You should now have a dataset that contains the building footprints of the insured risks and the name and distance of the nearest fire station. The core component of our simple model of wildfire risk will be the risk scores computed by **Precisely** as part of their **Fire Risk Pro** product.\n",
    "\n",
    "In contrast to other, similar products which offer risk scores across the cells of a square grid, Precisely have divided up their product area into 'fire sheds', analagous to the watersheds commonly used in the field of hydrology.\n",
    "\n",
    "From their documentation:\n",
    "> Many wildfire-related datasets are delivered in a format that divides the landscape into square grids. A square has very little to do with how a fire burns and the variation from square to square can be difficult to interpret. Fire Risk Pro takes a different approach.\n",
    "\n",
    "> All datasets Fire Risk Pro uses to create a final analysis (many of which come in “square” format) are integrated into polygons that reflect the landscape and how wildfires actually burn within that landscape. Fire Risk Pro divides the data into F and S fire sheds that are based on the topography (hills and valleys) of the landscape. These fire sheds tend to correlate to vegetation or wildland fuels and determine how a fire might burn and behave.\n",
    "\n",
    "> This means that fire sheds are more likely to divide geography into landscape-based units that exhibit similarities in vegetation and slope, and resultant fire behavior. The wildland and intermix modules of Fire Risk Pro ... use this concept of fire sheds to aggregate the landscape. tend to correlate to vegetation or wildland fuels and determine how a fire might burn and behave."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "36fd98f8-0761-486e-b550-43fd87d971bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "In order to include fire shed level attributes into our per-property model, we shall have to ascertain the fire shed(s) in which a given property is located; a join between two polygon datasets referred to as an intersection join.\n",
    "\n",
    "We have a spatial SQL function for performing exactly this kind of join `st_intersects`. Our preview documentation states:\n",
    "> `st_intersects(geom1, geom2)`\n",
    "> * **Input types:** (GEOMETRY,GEOMETRY) / (STRING, STRING) / (STRING, BINARY) / (BINARY, STRING) / (BINARY, BINARY)\n",
    "> * **Return type:** BOOLEAN\n",
    "> * **Description:** ST_Intersects takes as input two geometries and returns true iff the two geometries intersect.\n",
    "\n",
    "So if we wish to, we can just use the function directly in our join as a join condition. For a dataset of this size, it will take a little time but should complete without issue. Try it and see!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4d6889b8-ba88-470f-93a0-31466bc868c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "firesheds_tref = f\"precisely_wildfire_risk_usa_sample.riskdata.wfr_usa_fireriskpro\"\n",
    "\n",
    "fireshed_cols = [\"NOHARM_ID\", \"NOHARMCLS\", \"NOHARMODEL\", \"RISK50\", \"SEVERITY\", \"FREQUENCY\", \"WKT\"]\n",
    "\n",
    "firesheds_df = (\n",
    "  spark.table(firesheds_tref)\n",
    "  .select(*fireshed_cols)\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "df3112cd-7b57-4c75-9746-f41a0bfb41c1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Solution 4.1"
    }
   },
   "outputs": [],
   "source": [
    "# Naive join to obtain fireshed variables for each building\n",
    "\n",
    "with_fs_risk = (\n",
    "  closest_firestation.alias(\"b\")\n",
    "  .join(firesheds_df.alias(\"fs\"), on=F.expr(\"st_intersects(b.geometry, fs.WKT)\"))\n",
    "  )\n",
    "\n",
    "display(with_fs_risk.orderBy(\"RISK50\", ascending=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e12306ed-1378-4239-b744-68b3e931257b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "+ How might we account for the scenario where the property straddles more than one fireshed?\n",
    "\n",
    "  How could a filter be applied to your results in order to, for example, select the join match with the highest `RISK50` rating?\n",
    "    + **Hint** We already did something very similar when selecting the 'nearest' fire station."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d45e63c9-b8fd-49de-9e53-ab316bf2d9d1",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Solution 4.2"
    }
   },
   "outputs": [],
   "source": [
    "# Post-join filter\n",
    "ws_risk = Window.partitionBy(\"id\").orderBy(F.col(\"RISK50\").desc())\n",
    "\n",
    "buildings_with_unique_risk_rating = (\n",
    "  with_fs_risk\n",
    "  .withColumn(\"rank\", F.rank().over(ws_risk))\n",
    "  .where(F.col(\"rank\") == 1)\n",
    "  .drop(\"rank\")\n",
    "  )\n",
    "\n",
    "display(buildings_with_unique_risk_rating)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "2b4a2dc5-f483-4988-aabf-cf5caf079d50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def visualize_fireshed(results_df: pyspark.sql.dataframe.DataFrame, id: str):\n",
    "\n",
    "    filtered_pdf = (\n",
    "      results_df\n",
    "      .where(F.col(\"NOHARM_ID\") == id)\n",
    "      .drop(*fireshed_cols)\n",
    "      .toPandas()\n",
    "      )\n",
    "    \n",
    "    firestation = filtered_pdf[\"firestation_building_id\"][0]\n",
    "    firestation_geom_wkb = (\n",
    "      firestations_df\n",
    "      .where(F.col(\"firestation_building_id\") == firestation)\n",
    "      ).first()[\"firestation_geometry\"]\n",
    "    firestation_geom = shapely.wkb.loads(bytes(firestation_geom_wkb))\n",
    "    \n",
    "    filtered_pdf['geometry'] = gpd.GeoSeries.from_wkb(filtered_pdf['geometry'])\n",
    "    gdf = gpd.GeoDataFrame(filtered_pdf, geometry='geometry', crs='EPSG:4326')\n",
    "\n",
    "    fireshed_pdf = (\n",
    "      results_df\n",
    "      .where(F.col(\"NOHARM_ID\") == id)\n",
    "      .limit(1)\n",
    "      .select(*fireshed_cols)\n",
    "      .toPandas()\n",
    "      )\n",
    "\n",
    "    fireshed_pdf['geometry'] = gpd.GeoSeries.from_wkt(fireshed_pdf['WKT'])\n",
    "    gdf_fs = gpd.GeoDataFrame(fireshed_pdf, geometry='geometry', crs='EPSG:4326')\n",
    "\n",
    "    m = Map([\n",
    "      PolygonLayer.from_geopandas(gdf_fs, get_fill_color=[255, 0, 0], opacity=0.3),\n",
    "    ], show_tooltip=True)\n",
    "    m.add_layer(\n",
    "      PolygonLayer.from_geopandas(gdf, get_fill_color=[0, 0, 255]),\n",
    "      )\n",
    "    m.add_layer(\n",
    "      ScatterplotLayer.from_geopandas(\n",
    "        gpd.GeoDataFrame.from_records([[firestation_geom]])\n",
    "        .set_geometry(0)\n",
    "        .set_crs('EPSG:4326'),\n",
    "        radius_scale=50,\n",
    "        get_fill_color=[0, 255, 0]\n",
    "        ),\n",
    "    )\n",
    "    displayHTML(m.to_html())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fd8d6690-cc17-4c57-8ec9-8cf9fcc52757",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "visualize_fireshed(buildings_with_unique_risk_rating, 'CP1286009320')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cd9554c0-8445-4c53-9d4f-2ee677d45f7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "visualize_fireshed(buildings_with_unique_risk_rating, 'CX1302766484')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e6553428-ca4e-4bda-a77b-17d45d81d3b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### 4a: efficiency and scalability\n",
    "That same documentation also describes how you might look to employ a spatial grid indexing system in order to speed up the join and scale to much larger regions for your analysis.\n",
    "\n",
    "> Spatial predicates are more compute intensive. For performance purposes, we strongly urge customers to spatially index their data using productized functions `h3_lonlatash3` and `h3_tessellateaswkb`.\n",
    "\n",
    "In case you're not familiar with H3, you can read more about it [here](https://h3geo.org/). The tl;dr explanation would be along the lines of: \"geohashing with hexagons\".\n",
    "\n",
    "In our case, because both sides of the join contain polygon geometries, we should use `h3_tessellateaswkb` to first decompose the geometries into multiple smaller, simpler 'chips' (the pink polygons in the picture below) that are intersections of the original geometry (red filled area) H3 grid indexing system (black hexagons).\n",
    "\n",
    "<img src=\"assets/h3-tessellation.png\" width=\"450px\" title=\"blah\">\n",
    "\n",
    "*Decomposition of a Precisely fire shed at an H3 resolution of 9.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f96d0ac-4823-4b33-93bc-e4861d54c662",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "The `h3_tessellateaswkb` function is documented thus:\n",
    "> `h3_tessellateaswkb(geom, resolution, [return_core_chips])`\n",
    "> * **Input types:** (STRING, INT[, BOOLEAN])/(GEOGRAPHY, INT[, BOOLEAN])/(BINARY, INT[, BOOLEAN])\n",
    "> * **Return type:** STRUCT{BIGINT, BOOLEAN, BINARY}\n",
    "> * **Description:** Tessellate input vector data using h3 global grid indexing.\n",
    ">\n",
    ">    Takes as input two required and one optional argument:\n",
    ">   * A geography object (either a GEOGRAPHY value, or a STRING/BINARY value representing a geography in GeoJSON, WKB, or WKT format).\n",
    ">   * A resolution value (value between 0 and 15, inclusive).\n",
    ">   * [Optional] A boolean value indicating whether the WKB description of core H3 cells should be present in the output, or replaced by NULL values (see below).\n",
    ">\n",
    ">   The function returns an array of named structs. The array represents a tessellation of the input geography using H3 cells at the specified resolution. The tessellation is essentially a decomposition of the geography using a minimal covering set of H3 cells.\n",
    ">\n",
    ">   The structs have three fields: \"cellid\", \"core\", \"chip\".\n",
    ">   * The \"cellid\" is an H3 cell at the specified resolution that is a member of the minimal covering set of H3 cells of the geography at the specified resolution.\n",
    ">   * The \"core\" field is a boolean value that indicates whether the H3 cell of the struct is fully contained in the geography (for core cells the intersection of the geography with the cell's polygon is the cell's polygon).\n",
    ">   * The \"chip\" field is a BINARY value, and corresponds to the intersection of the input geography with the cell's polygon, represented in WKB format. If the case where the cell is fully contained inside the geography and the third optional argument is set to false, the \"chip\" field value would be NULL instead."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d7c7ab8c-c734-441d-915c-76a457ac8992",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Once we decompose the geometries on both our left and right sides of the join, we can then perform the join as an equi-join (exact matches between keys) between H3 indexes, before (if necessary) filtering out false-positives using `st_intersects` between the property geometry and the fire shed chip, e.g.\n",
    "\n",
    "```sql\n",
    "SELECT\n",
    "...\n",
    "FROM properties p\n",
    "  INNER JOIN indexed_firesheds f\n",
    "  ON p.h3 = f.h3\n",
    "WHERE (\n",
    "  f.core \n",
    "  OR st_intersects(p.geometry, f.chip)\n",
    "  )\n",
    "```\n",
    "\n",
    "Go ahead and try it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5b75bdd8-4863-4990-8482-17cc10d61da3",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Solution 4.3"
    }
   },
   "outputs": [],
   "source": [
    "firesheds_indexed = (\n",
    "  firesheds_df\n",
    "  .withColumn(\"chip\", F.expr(\"explode(h3_tessellateaswkb(WKT, 9))\"))\n",
    "  .select(*fireshed_cols, \"chip.*\")\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d01905a-61bc-415c-8a8d-476387dccbd5",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Solution 4.4"
    }
   },
   "outputs": [],
   "source": [
    "with_fs_risk_h3 = (\n",
    "  closest_firestation\n",
    "  .withColumn(\"h3\", F.expr(\"explode(h3_tessellateaswkb(geometry, 9))\"))\n",
    "  .select(*building_cols, *firestation_cols, \"h3.*\")\n",
    "  .alias(\"b\")\n",
    "  .join(firesheds_indexed.alias(\"fs\"), on=\"cellid\")\n",
    "  .where(\"st_geometrytype(fs.chip) = 'ST_Polygon'\")\n",
    "  .where(\n",
    "    F.col(\"fs.core\") |\n",
    "    F.expr(\"st_intersects(b.geometry, fs.chip)\")\n",
    "    )\n",
    "  .select(*building_cols, *firestation_cols, *fireshed_cols)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f865f4ee-a9e3-4e0f-a542-c92b941d41ba",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Solution 4.5"
    }
   },
   "outputs": [],
   "source": [
    "ws_risk = Window.partitionBy(\"id\").orderBy(F.col(\"RISK50\").desc())\n",
    "\n",
    "buildings_with_unique_risk_rating_h3 = (\n",
    "  with_fs_risk_h3\n",
    "  .distinct()\n",
    "  .withColumn(\"rank\", F.rank().over(ws_risk))\n",
    "  .where(F.col(\"rank\") == 1)\n",
    "  .drop(\"rank\")\n",
    ")\n",
    "\n",
    "display(with_fs_risk_h3.orderBy(F.col(\"RISK50\").desc()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -7575235587420523,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24ecccc2-ed8f-4363-b576-1b7dc2fe4a9b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "visualize_fireshed(buildings_with_unique_risk_rating_h3, \"CP1286009320\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -7575235587420523,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "be52bf3f-d287-4849-98e2-9f76f54741e1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "visualize_fireshed(buildings_with_unique_risk_rating_h3, \"CX1302766484\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f7aa40e0-7ec2-407c-948d-63509f16bc31",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Sub-task (5): Add weather into the model\n",
    "\n",
    "Take a look at the notebook `custom_spark_data_source_ifs` in the folder `./utils`.\n",
    "\n",
    "Inside, there's an example implementation of a custom Pyspark DataSource that can be used to obtain weather forecast data from the ECMWF.\n",
    "\n",
    "As mentioned in the introductory section of this notebook, this data is provided in 0.25° grids in GRIB format.\n",
    "\n",
    "We could simply download this data from the ECMWF Open Data Portal, interrogate it using a tool such as [xarray](https://xarray.dev/) and extract the values to support exactly the kind of analysis we are intending to perform here.\n",
    "\n",
    "This would quickly become difficult to manage if looking across multiple forecasts, variables and regions. For that type and scale of application, we can take an xarray based workflow and wrap it inside a Spark DataSource reader using the new [pyspark.sql.datasource.DataSource](https://spark.apache.org/docs/4.0.0-preview2/api/python/reference/pyspark.sql/api/pyspark.sql.datasource.DataSource.html) class.\n",
    "\n",
    "A more thorough examination of the topic is offered [here](https://docs.databricks.com/aws/en/pyspark/datasources) but we can use this implementation to obtain forecast data that can be joined to our dataset as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d861abd-1959-4bdf-9962-32512d639d9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Reading ECMWF forecasts using Spark and storing the results into UC\n",
    "Reading GRIB files with xarray requires a native code dependency installed through the `cfgrib` python package. We use this command to check that it has been correctly installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -7575235587420523,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3e5c1bb0-9cbd-4f52-9a1d-5091e908a68a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sh python -m cfgrib selfcheck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -7575235587420523,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "89e27876-d9b2-4ebc-beab-31983fee0f73",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%run ./utils/custom_spark_data_source_ifs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -7575235587420523,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e5da00b5-c17f-4c4c-97aa-c717273cfc7d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.dataSource.register(IFSDataSource)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "60491b54-760b-482c-9419-ea5177c72760",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Data filtering\n",
    "##### Spatial filtering\n",
    "Sadly, we can't speed up the download process by filtering with a spatial boundary. Instead, we will read all values and drop any out-of-bounds rows before writing into Delta.\n",
    "##### Time horizon\n",
    "By default, we will download the forecast produced yesterday, since that is guaranteed to be available at any point during the day. Current day forecasts from the 0h UTC run can take up to 9 hours to be published to the open data store."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -7575235587420523,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "59043929-4d17-4d2f-84eb-e2b1bdf6258c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "forecast_date = (\n",
    "  datetime.now() - timedelta(days=1) # D - 1\n",
    "  ).strftime('%Y%m%d')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -7575235587420523,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "14dab4bb-b21d-47ea-bd7f-c8cb44b425b2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather_df = (\n",
    "  spark.read.format(\"ifs\")\n",
    "  .options(**{\n",
    "    \"forecastDate\": forecast_date,\n",
    "    # download the midnight model run\n",
    "    \"forecastTime\": 0,\n",
    "    # only process subset of forecast steps (forecast horizon in hours)\n",
    "    #  adjust to your needs (more hours, more spark tasks to be completed)\n",
    "    \"forecastHorizon\": \"3\",\n",
    "    # extract temperature, 'u' and 'v' components of windspeed and total rainfall\n",
    "    \"variables\": \"t2m,u100,v100,tp\"\n",
    "  })\n",
    "  .load()\n",
    "  )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -7575235587420523,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc79017f-7efc-4ed7-b84b-e2bf5e8e5e91",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(weather_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -7575235587420523,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "943155fa-c9e4-4550-814a-d18fb95a0d89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "weather_data_tref = f\"{CATALOG}.{SCHEMA}.weather_forecast_raw\"\n",
    "print(f'Saving ifs data to: {weather_data_tref}')\n",
    "\n",
    "(\n",
    "  weather_df\n",
    "  .where(F.col(\"x\").between(math.floor(xmin), math.ceil(xmax)))\n",
    "  .where(F.col(\"y\").between(math.floor(ymin), math.ceil(ymax)))\n",
    "  .write.saveAsTable(weather_data_tref, mode=\"overwrite\")\n",
    ")\n",
    "\n",
    "forecast_df = spark.table(weather_data_tref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -7575235587420523,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9968a490-c11b-4af9-b89e-46761fb97b08",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": null,
       "filterBlob": "{\"filterGroups\":[],\"syncTimestamp\":1744893737847}",
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": [
       "Databricks visualization. Run in Databricks to view."
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1.subcommand+json": {
       "baseErrorDetails": null,
       "bindings": {},
       "collapsed": false,
       "command": "%python\n__backend_agg_display_orig = display\n__backend_agg_dfs = []\ndef __backend_agg_display_new(df):\n    __backend_agg_df_modules = [\"pandas.core.frame\", \"databricks.koalas.frame\", \"pyspark.sql.dataframe\", \"pyspark.pandas.frame\", \"pyspark.sql.connect.dataframe\"]\n    if (type(df).__module__ in __backend_agg_df_modules and type(df).__name__ == 'DataFrame') or isinstance(df, list):\n        __backend_agg_dfs.append(df)\n\ndisplay = __backend_agg_display_new\n\ndef __backend_agg_user_code_fn():\n    import base64\n    exec(base64.standard_b64decode(\"ZGlzcGxheShmb3JlY2FzdF9kZi53aGVyZSgidmFyaWFibGU9J3QybSciKSk=\").decode())\n\ntry:\n    # run user code\n    __backend_agg_user_code_fn()\n\n    #reset display function\n    display = __backend_agg_display_orig\n\n    if len(__backend_agg_dfs) > 0:\n        # create a temp view\n        if type(__backend_agg_dfs[0]).__module__ == \"databricks.koalas.frame\":\n            # koalas dataframe\n            __backend_agg_dfs[0].to_spark().createOrReplaceTempView(\"DatabricksView10446f0\")\n        elif type(__backend_agg_dfs[0]).__module__ == \"pandas.core.frame\" or isinstance(__backend_agg_dfs[0], list):\n            # pandas dataframe\n            spark.createDataFrame(__backend_agg_dfs[0]).createOrReplaceTempView(\"DatabricksView10446f0\")\n        else:\n            __backend_agg_dfs[0].createOrReplaceTempView(\"DatabricksView10446f0\")\n        #run backend agg\n        display(spark.sql(\"\"\"WITH q AS (select * from DatabricksView10446f0) SELECT `valid_time`,AVG(`m`) `column_722c0bed109`,MIN(`m`) `column_722c0bed116`,MAX(`m`) `column_722c0bed119` FROM q GROUP BY `valid_time`\"\"\"))\n    else:\n        displayHTML(\"dataframe no longer exists. If you're using dataframe.display(), use display(dataframe) instead.\")\n\n\nfinally:\n    spark.sql(\"drop view if exists DatabricksView10446f0\")\n    display = __backend_agg_display_orig\n    del __backend_agg_display_new\n    del __backend_agg_display_orig\n    del __backend_agg_dfs\n    del __backend_agg_user_code_fn\n\n",
       "commandTitle": "Visualization 1",
       "commandType": "auto",
       "commandVersion": 0,
       "commentThread": [],
       "commentsVisible": false,
       "contentSha256Hex": null,
       "customPlotOptions": {
        "redashChart": [
         {
          "key": "type",
          "value": "CHART"
         },
         {
          "key": "options",
          "value": {
           "alignYAxesAtZero": true,
           "coefficient": 1,
           "columnConfigurationMap": {
            "x": {
             "column": "valid_time",
             "id": "column_722c0bed124"
            },
            "y": [
             {
              "column": "m",
              "id": "column_722c0bed109",
              "transform": "AVG"
             },
             {
              "column": "m",
              "id": "column_722c0bed116",
              "transform": "MIN"
             },
             {
              "column": "m",
              "id": "column_722c0bed119",
              "transform": "MAX"
             }
            ]
           },
           "dateTimeFormat": "DD/MM/YYYY HH:mm",
           "direction": {
            "type": "counterclockwise"
           },
           "error_y": {
            "type": "data",
            "visible": true
           },
           "globalSeriesType": "line",
           "isAggregationOn": true,
           "legend": {
            "traceorder": "normal"
           },
           "missingValuesAsZero": true,
           "numberFormat": "0,0.[00000]",
           "percentFormat": "0[.]00%",
           "series": {
            "error_y": {
             "type": "data",
             "visible": true
            },
            "stacking": null
           },
           "seriesOptions": {
            "column_722c0bed109": {
             "type": "line",
             "yAxis": 0
            },
            "column_722c0bed116": {
             "type": "line",
             "yAxis": 0
            },
            "column_722c0bed119": {
             "type": "line",
             "yAxis": 0
            }
           },
           "showDataLabels": false,
           "sizemode": "diameter",
           "sortX": true,
           "sortY": true,
           "swappedAxes": false,
           "textFormat": "",
           "useAggregationsUi": true,
           "valuesOptions": {},
           "version": 2,
           "xAxis": {
            "labels": {
             "enabled": true
            },
            "type": "-"
           },
           "yAxis": [
            {
             "type": "-"
            },
            {
             "opposite": true,
             "type": "-"
            }
           ]
          }
         }
        ]
       },
       "datasetPreviewNameToCmdIdMap": {},
       "diffDeletes": [],
       "diffInserts": [],
       "displayType": "redashChart",
       "error": null,
       "errorDetails": null,
       "errorSummary": null,
       "errorTraceType": null,
       "finishTime": 0,
       "globalVars": {},
       "guid": "",
       "height": "auto",
       "hideCommandCode": false,
       "hideCommandResult": false,
       "iPythonMetadata": null,
       "inputWidgets": {},
       "isLockedInExamMode": false,
       "latestUser": "a user",
       "latestUserId": null,
       "listResultMetadata": null,
       "metadata": {
        "byteLimit": 2048000,
        "rowLimit": 10000
       },
       "nuid": "b26c7a00-e904-4860-9ba6-548202df2643",
       "origId": 0,
       "parentHierarchy": [],
       "pivotAggregation": null,
       "pivotColumns": null,
       "position": 16.48828125,
       "resultDbfsErrorMessage": null,
       "resultDbfsStatus": "INLINED_IN_TREE",
       "results": null,
       "showCommandTitle": false,
       "startTime": 0,
       "state": "input",
       "streamStates": {},
       "subcommandOptions": {
        "queryPlan": {
         "groups": [
          {
           "column": "valid_time",
           "type": "column"
          }
         ],
         "selects": [
          {
           "column": "valid_time",
           "type": "column"
          },
          {
           "alias": "column_722c0bed109",
           "args": [
            {
             "column": "m",
             "type": "column"
            }
           ],
           "function": "AVG",
           "type": "function"
          },
          {
           "alias": "column_722c0bed116",
           "args": [
            {
             "column": "m",
             "type": "column"
            }
           ],
           "function": "MIN",
           "type": "function"
          },
          {
           "alias": "column_722c0bed119",
           "args": [
            {
             "column": "m",
             "type": "column"
            }
           ],
           "function": "MAX",
           "type": "function"
          }
         ]
        }
       },
       "submitTime": 0,
       "subtype": "tableResultSubCmd.visualization",
       "tableResultIndex": 0,
       "tableResultSettingsMap": {},
       "useConsistentColors": false,
       "version": "CommandV1",
       "width": "auto",
       "workflows": null,
       "xColumns": null,
       "yColumns": null
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(forecast_df.where(\"variable='t2m'\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "33308fc7-a1ad-44d7-b6f1-df8fab7e5257",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "From this point, it is completely up to you to decide how include this data in your analysis. \n",
    "\n",
    "Here are some things you may want to consider:\n",
    "+ How will you join this data to your current dataset? Do you need to perform a point-in-polyon join using `st_contains`? Or is it good enough to extract the lons and lats of each building and round these to the nearest quarter degree?\n",
    "+ Will the data need to be aggregated after joining? If, for example, you choose to align the data to fire sheds rather than properties, then it's quite possible you will have more than one data point for each fire shed.\n",
    "+ What is the relative importance of temperature vs. wind speed? Does the direction of the wind matter, if so, how will you account for this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -7575235587420523,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "052d55bb-f2ac-49b6-8ebd-e16e939b5105",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "with_weather = ( ... )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -7575235587420523,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92eb3205-d26c-4117-8812-31aee926f6a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.notebook.exit(\"0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "batchId": -7575235587420523,
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "collapsed": true,
     "inputWidgets": {},
     "nuid": "5b09f78a-cf67-47c5-8647-0f63db500457",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "fireshed_pdf = (\n",
    "      firesheds_df\n",
    "      .where(F.col(\"NOHARM_ID\") == \"CP1286009320\")\n",
    "      .toPandas()\n",
    "      )\n",
    "\n",
    "fireshed_indexed_pdf = (\n",
    "      firesheds_indexed\n",
    "      .where(F.col(\"NOHARM_ID\") == \"CP1286009320\")\n",
    "      .withColumn(\"h3_geom\", F.expr(\"h3_boundaryaswkb(cellid)\"))\n",
    "      .toPandas()\n",
    "      )\n",
    "\n",
    "chips_pdf = fireshed_indexed_pdf.copy(deep=True)\n",
    "\n",
    "fireshed_pdf['fs_geometry'] = gpd.GeoSeries.from_wkt(fireshed_pdf['WKT'])\n",
    "fireshed_indexed_pdf['h3_geometry'] = gpd.GeoSeries.from_wkb(fireshed_indexed_pdf['h3_geom'])\n",
    "chips_pdf['chip_geometry'] = gpd.GeoSeries.from_wkb(chips_pdf['chip'])\n",
    "\n",
    "gdf_fs = gpd.GeoDataFrame(fireshed_pdf, geometry='fs_geometry', crs='EPSG:4326')\n",
    "gdf_h3 = gpd.GeoDataFrame(fireshed_indexed_pdf, geometry='h3_geometry', crs='EPSG:4326')\n",
    "gdf_chip = gpd.GeoDataFrame(chips_pdf, geometry='chip_geometry', crs='EPSG:4326')\n",
    "\n",
    "m = Map([\n",
    "  PolygonLayer.from_geopandas(gdf_fs, get_fill_color=[255, 0, 0], opacity=0.3),\n",
    "  PolygonLayer.from_geopandas(gdf_h3, filled=False, get_line_width=5),\n",
    "  PolygonLayer.from_geopandas(gdf_chip, filled=False, get_line_color=[255, 0, 255], get_line_width=10),\n",
    "])\n",
    "displayHTML(m.to_html())\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7575235587420527,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Lab solutions",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
