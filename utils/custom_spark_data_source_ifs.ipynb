{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c2c43f38-17af-4ffe-b0df-41b722e166ac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import requests\n",
    "import tempfile\n",
    "import xarray as xr\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "from urllib.parse import urlparse\n",
    "\n",
    "from pyspark.sql.datasource import DataSource, DataSourceReader, InputPartition\n",
    "import pyspark.sql.types as ST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "45e3af0a-a8f9-49c1-80f0-fc13aa4f5fee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class GRIBMessagePartition(InputPartition):\n",
    "  def __init__(self, path: str):\n",
    "    self.path = path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ea0e4c15-9b57-4865-8774-6fd112a2a3e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class IFSDataSourceReader(DataSourceReader):\n",
    "  def __init__(self, schema, options):    \n",
    "    self.schema: ST.StructType = schema\n",
    "    self.options = options\n",
    "  \n",
    "  @property\n",
    "  def forecast_time(self):\n",
    "    forecast_time_str: str = self.options.get(\"forecastTime\")\n",
    "    if not forecast_time_str:\n",
    "      raise ValueError(\"The 'forecastTime' option is required.\")\n",
    "    return int(forecast_time_str)\n",
    "  \n",
    "  @property\n",
    "  def forecast_date(self):\n",
    "    forecast_date: str = self.options.get(\"forecastDate\")\n",
    "    if not forecast_date:\n",
    "      raise ValueError(\"The 'forecastDate' option is required.\")\n",
    "    return forecast_date\n",
    "  \n",
    "  @property\n",
    "  def forecast_url(self):\n",
    "    return (\n",
    "      \"https://data.ecmwf.int/forecasts/\"\n",
    "      f\"{self.forecast_date}/\"\n",
    "      f\"{self.forecast_time:02}z/\"\n",
    "      \"ifs/0p25/oper/\"\n",
    "      )\n",
    "  \n",
    "  @property\n",
    "  def root_url(self):\n",
    "    parsed_url = urlparse(self.forecast_url)\n",
    "    return f\"{parsed_url.scheme}://{parsed_url.netloc}\"\n",
    "  \n",
    "  @property\n",
    "  def input_files(self):\n",
    "    r = requests.get(self.forecast_url)\n",
    "    r.raise_for_status()\n",
    "    soup = BeautifulSoup(r.text, 'html.parser')\n",
    "    all_urls = soup.find_all('a', href=True)\n",
    "    grib_urls = [f\"{self.root_url}{a['href']}\" for a in all_urls if a['href'].endswith('.grib2')]\n",
    "    return grib_urls\n",
    "  \n",
    "  @property\n",
    "  def variables(self):\n",
    "    return self.options.get(\"variables\").split(\",\")\n",
    "  \n",
    "  def partitions(self):\n",
    "    parts = []\n",
    "    for path in self.input_files:\n",
    "      parts.append(GRIBMessagePartition(path))\n",
    "    return parts\n",
    "\n",
    "  def read(self, partition):\n",
    "    with tempfile.NamedTemporaryFile(mode=\"w+b\", suffix=\".grib2\") as fn:\n",
    "      r = requests.get(partition.path, stream=True)\n",
    "      r.raise_for_status()\n",
    "      for chunk in r.iter_content(chunk_size=8192):\n",
    "        fn.write(chunk)\n",
    "      var_dfs = []\n",
    "      for var in self.variables:\n",
    "        print(f\"{partition=}, {var=}\")\n",
    "        with xr.open_dataset(fn.name, filter_by_keys={\"cfVarName\": var}) as ds:\n",
    "          col_subset = [\"time\", \"valid_time\", \"latitude\", \"longitude\", var]\n",
    "          var_pdf = ds.to_dataframe().reset_index(drop=False)[col_subset]\n",
    "          var_pdf[\"time\"] = var_pdf[\"time\"].dt.tz_localize(\"utc\")\n",
    "          var_pdf[\"valid_time\"] = var_pdf[\"valid_time\"].dt.tz_localize(\"utc\")\n",
    "          var_pdf[\"variable\"] = var\n",
    "          var_pdf[\"value\"] = var_pdf[var].astype('float32')\n",
    "          var_pdf[\"path\"] = partition.path\n",
    "          output_cols = [\"time\", \"valid_time\", \"variable\", \"longitude\", \"latitude\", \"value\", \"path\"]\n",
    "          var_dfs.append(var_pdf[output_cols])\n",
    "      output_pdf = pd.concat(var_dfs, ignore_index=True)\n",
    "    yield from output_pdf.itertuples(index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "837ddd14-7218-467f-b890-2ed117cd68b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class IFSDataSource(DataSource):\n",
    "\n",
    "    @classmethod\n",
    "    def name(cls) -> str:\n",
    "        \"\"\"\n",
    "        Get the name of the data source.\n",
    "\n",
    "        Returns:\n",
    "            str: The name of the data source.\n",
    "        \"\"\"\n",
    "        return \"ifs\"\n",
    "    \n",
    "    def schema(self) -> ST.StructType:\n",
    "        \"\"\"\n",
    "        Define the schema for the output data.\n",
    "\n",
    "        Returns:\n",
    "            StructType: The schema including fields for the variable identifier, band index,\n",
    "            metadata, coordinates, and values.\n",
    "        \"\"\"\n",
    "        return ST.StructType([\n",
    "            ST.StructField(\"time\", ST.TimestampType(), True),\n",
    "            ST.StructField(\"valid_time\", ST.TimestampType(), True),\n",
    "            ST.StructField(\"variable\", ST.StringType(), True),\n",
    "            ST.StructField(\"x\", ST.DoubleType(), True),\n",
    "            ST.StructField(\"y\", ST.DoubleType(), True),\n",
    "            ST.StructField(\"m\", ST.DoubleType(), True),\n",
    "            ST.StructField(\"path\", ST.StringType(), True),\n",
    "        ])\n",
    "\n",
    "    def reader(self, schema: ST.StructType):\n",
    "        return IFSDataSourceReader(schema, self.options)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "custom_spark_data_source_ifs",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
